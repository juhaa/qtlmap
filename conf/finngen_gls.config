/*
 * ---------------------------------------------------------------------------
 *  Nextflow config file for use with Google Cloud & Google Life Sciences API
 * ---------------------------------------------------------------------------
 * Defines basic settings for running under the finngen-refinery-dev project
 */

workDir = "$TOWER_WORKDIR"

params {
  // Workflow flags
  // TODO nf-core: Specify your pipeline's command line flags

  mincisvariant = 5 // Minimum count of cis variants in cis-distance of gene to be taken into account.
  is_imputed = true
  n_batches = 400
  cis_window = 1000000 // number of basepairs from the beginning of the gene to both directions
  n_permutations = 1000
  vcf_has_R2_field = false

  run_permutation = false
  run_nominal = true
  reformat_sumstats = true
  outdir = 'gs://juham-fg/eQTL/testing/'

  cs_size_threshold = 200

  //covariates
  n_geno_pcs = 6
  n_pheno_pcs = 6
  covariates = "null"

  //SuSiE
  run_susie = true
  vcf_genotype_field = 'GT'
  susie_skip_full = false

  readPaths = false
  // Boilerplate options
  name = false //Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.
  email = false
  plaintext_email = false
  help = false
  igenomes_base = "./iGenomes"
  tracedir = "${params.outdir}/pipeline_info/${params.name}"
  clusterOptions = false
  awsqueue = false
  awsregion = 'eu-west-1'
  igenomesIgnore = true
  custom_config_version = 'master'
}

tower {
  accessToken = "$TOWER_ACCESS_TOKEN"
  endpoint = 'http://0.0.0.0:8000/api'
  enabled = true
}

mail {
  from = "$TOWER_MAIL_FROM"
  smtp {
    host = "$TOWER_MAIL_HOST"
    port = "$TOWER_MAIL_PORT"
    user = "$TOWER_MAIL_USER"
    password = "$TOWER_MAIL_PASS"
  }
}

executor {
  queueSize = queueSize = 100000
}

google {
  region  = 'europe-west1'
  location = 'europe-west4'
  project = "$TOWER_GOOGLE_PROJECT"
  lifeSciences {
    serviceAccountEmail = "$TOWER_GOOGLE_SERVACC"
    network = 'default'
    subnetwork = 'default'
    // boot disk could be smaller e.g. 10G if the docker image for the process fits there
    // some docker images are too big for a 10G boot disk
    bootDiskSize = 15.GB
    preemptible = true
    usePrivateAddress = true
    sshDaemon = false
    keepAliveOnFailure = false
    debug = false
  }
  storage {
    maxParallelTransfers = 10
    parallelThreadCount = 4
    downloadMaxComponents = 8
    delayBetweenAttempts = 300
  }
}

//Get valid amount of cpus/memory
def get_valid(value, type) {
  //Google Cloud VM limitations for N1 machine series
  min_memory = 1.GB
  max_memory = 624.GB
  max_cpus = 96
  min_cpus = 1
  if (type == 'memory') {
    try {
      if (value.compareTo(max_memory as nextflow.util.MemoryUnit) == 1) {
        return max_memory as nextflow.util.MemoryUnit
      } else if (value.compareTo(min_memory as nextflow.util.MemoryUnit) == -1) {
        return min_memory as nextflow.util.MemoryUnit
      } else {
        return value
      }
    } catch (all) {
      println "   ### ERROR ###   Max memory '${max_memory}' is not valid! Using default value: $value"
      return value
    }
  } else if (type == 'cpus') {
    try {
      value = Math.min(value, max_cpus)
      value = Math.max(value, min_cpus)
      if (value > min_cpus) {
        value += (value % 2)
      }
      return value
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${max_cpus}' is not valid! Using default value: $value"
      return value
    }
  }
}

//Get machineType string according to Google Cloud limitations
def get_machineType(cpus, mem) {
  // Google VM mem/cpu ratio limits
  min_ratio = 1
  max_ratio = 6.5
  cpus = get_valid(cpus, "cpus")
  mem = get_valid(mem, "memory")
  ratio = mem.toGiga() / cpus
  if (ratio < min_ratio) {
    mem = [cpus, "GB"].join(".") as nextflow.util.MemoryUnit
  }
  machineType = ["custom", cpus, mem.getMega()].join("-")
  if (ratio > max_ratio) {
    machineType = [machineType, "ext"].join("-")
  }
  return machineType
}

process {
  executor = 'google-lifesciences'
  cache = 'lenient'

  // retry when preempted (10) or aborted/preempted (14) or weird 127 possibly file copy issue (127)
  // otherwise terminate workflow allowing submitted tasks to finish
  // https://cloud.google.com/life-sciences/docs/troubleshooting
  //errorStrategy = { task.exitStatus in [10,14,127,137] ? 'retry' : 'finish' }
  //maxRetries = 3

  errorStrategy = { task.exitStatus in [1] ? 'terminate' : 'retry' }
  maxRetries = 3
  maxErrors = '-1'

  cpus = 2
  memory = { 8.GB * task.attempt }

  // Process-specific resource requirements
  withName: 'extract_variant_info {  '  
    machineType = {get_machineType(1, 1.GB * task.attempt)}
  }
  withName: 'prepare_molecular_traits' {
    machineType = {get_machineType(1, 20.GB * task.attempt)}
  }
  withName: 'compress_bed' {
    machineType = {get_machineType(1, 1.GB * task.attempt)}
  }
  withName: 'extract_samples_from_vcf' {
    machineType = {get_machineType(1, 1.GB * task.attempt)}
  }
  withName: 'make_pca_covariates' {
    machineType = {get_machineType(4, 12.GB * task.attempt)}
  }
  withName: 'run_permutation' {
    machineType = {get_machineType(1, 5.GB * task.attempt)}
  }  
  withName: 'run_nominal' {
    machineType = {get_machineType(1, 5.GB * task.attempt)}
  }
  withName: 'merge_nominal_batches' {
   machineType = {get_machineType(1, 8.GB * task.attempt)}
  }
  withName: 'merge_permutation_batches' {
    machineType = {get_machineType(1, 5.GB * task.attempt)}
  }
  withName: 'sort_qtltools_output' {
    machineType = {get_machineType(10, 32.GB * task.attempt)}
  }
  withName: 'join_rsids_var_info' {
    machineType = {get_machineType(2, 8.GB * task.attempt)}
  }
  withName: 'reformat_sumstats' {
    machineType = {get_machineType(2, 32.GB * task.attempt)}
  }
  withName: 'tabix_index' {
    machineType = {get_machineType(1, 1.GB * task.attempt)}
  }  
  withName: 'run_susie' {
    machineType = {get_machineType(2, 24.GB * task.attempt)}
  }
  withName: 'merge_cs_sumstats' {
    machineType = {get_machineType(2, 24.GB * task.attempt)}
  }
  withName: 'extract_lead_cc_signal' {
    machineType = {get_machineType(2, 32.GB * task.attempt)}
  }
}
